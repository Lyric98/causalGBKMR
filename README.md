### Performance Optimization

```r
# For large datasets and complex models
optimize_gbkmr <- function(sim_popn, T, Adim, Ldim) {
  
  # Recommended settings based on data complexity
  if (nrow(sim_popn) > 10000 && Adim * T > 10) {
    # Large, complex data
    settings <- list(
      n = min(1000, nrow(sim_popn) * 0.1),    # Sample 10% or max 1000
      iter = 20000,                            # More iterations for stability
      sel = seq(15000, 20000, by = 25),       # Conservative burn-in
      use_knots = TRUE,                        # Essential for efficiency
      n_knots = 100,                          # More knots for complex relationships
      K = 2000                                # More prediction samples
    )
  } else if (Adim * Ldim * T > 20) {
    # High-dimensional but smaller sample
    settings <- list(
      n = min(500, nrow(sim_popn) * 0.2),
      iter = 15000,
      sel = seq(10000, 15000, by = 20),
      use_knots = TRUE,
      n_knots = 75,
      K = 1500
    )
  } else {
    # Standard settings
    settings <- list(
      n = min(500, nrow(sim_popn)),
      iter = 12000,
      sel = seq(8000, 12# g-Bayesian Kernel Machine Regression (g-BKMR) Package
```
In this document, we illustrate the main features of the `causalGBKMR` R package through examples. This approach enables causal inference for health effects of time-varying correlated environmental mixtures while accounting for time-varying confounding.

## Cite the method

The g-BKMR method is introduced in the paper:

Chai, Z., Navas-Acien, A., Coull, B., & Valeri, L. (2024). g-BKMR: Causal Inference for health effects of time-varying correlated environmental mixtures. *Journal of Statistical Methods*.

## g-BKMR Method Overview

g-BKMR extends Bayesian Kernel Machine Regression (BKMR) to longitudinal settings with time-varying exposures and confounders. The method combines:

- **Flexible modeling**: Non-linear, non-additive effects of environmental mixtures
- **Causal inference**: G-formula approach for time-varying confounding
- **Variable selection**: Bayesian variable selection for high-dimensional exposures

The general framework models:
- Time-varying confounders: $L_t = h_L(A_{t-1}, L_{t-1}, C_0) + \epsilon_{L_t}$
- Outcome: $Y = h_Y(A_0, ..., A_{T-1}, L_1, ..., L_{T-1}, C_0) + \epsilon_Y$

where $A_t$ represents exposures at time $t$, $L_t$ are time-varying confounders, and $C_0$ are baseline covariates.

## Installation

```r
# Install required packages
install.packages(c("bkmr", "dplyr", "ggplot2", "parallel", 
                   "fields", "lubridate", "data.table", "corrplot"))

# Install causalGBKMR package (development version)
devtools::install_github("your-username/causalGBKMR")
library(causalGBKMR)
```

## Data Generation

To illustrate the main features of the R package `causalGBKMR`, let's first generate some data. We have built in a few functions directly into the R package for this purpose.

```r
library(causalGBKMR)

# Generate population data
set.seed(123)
sim_popn <- generate_panel_data(
  popN = 1e6,                # Population size  
  T = 3,                     # Number of time points
  Adim = 3,                  # Number of exposures per time point
  Ldim = 3,                  # Number of time-varying confounders per time point
  outcome_type = "binary",   # "continuous" or "binary"
  relationship_type = "quadratic",  # "linear", "quadratic", or "quadratic+interaction"
  confounding = "high"       # "low" or "high"
)

# Examine the generated data structure
head(sim_popn)
colnames(sim_popn)
```

**Key parameters for data generation:**
- `popN`: Population size for simulation
- `T`: Number of time points (including baseline)
- `Adim`: Number of exposure variables at each time point
- `Ldim`: Number of time-varying confounders at each time point  
- `outcome_type`: Type of outcome variable
- `relationship_type`: Functional form of exposure-outcome relationship
- `confounding`: Strength of confounding relationships

The generated dataset will have columns following this pattern:
- Baseline variables: `sex`, `waist0_1`, ..., `waist0_Ldim` 
- Exposures: `logM1_0`, ..., `logMAdim_0`, `logM1_1`, ..., `logMAdim_T-1`
- Time-varying confounders: `waist1_1`, ..., `waist1_Ldim`, ..., `waistT-1_Ldim`
- Outcome: `Y`
- Subject ID: `id`

## Data Format Requirements

causalGBKMR works with the **wide-format data** generated by `generate_panel_data()` or similar user data.

### Required Data Structure

The package expects data in wide format where:

1. **Time Variable**: Implicitly represented through variable naming (e.g., `_0`, `_1`, `_2` suffixes)
2. **Multiple Exposures**: Multiple exposure variables per time point (e.g., `logM1_0`, `logM2_0`, `logM3_0` for 3 exposures at time 0)
3. **Time-Dependent Confounders**: Multiple confounders per time point (e.g., `waist1_1`, `waist2_1`, `waist3_1` for 3 confounders at time 1)
4. **Baseline Covariates**: Time-invariant variables (e.g., `sex`, `waist0_1`, `waist0_2`)
5. **Outcome**: Single outcome variable `Y` measured at the final time point

## Tutorial

### Basic Usage (Novice Mode)

For users who want a simple analysis with the generated data:

```r
library(causalGBKMR)

# Run g-BKMR analysis on generated data
results_basic <- run_gbkmr_panel(
  sim_popn = sim_popn,         # Generated population data
  T = 3,                       # Number of time points
  currind = 1,                 # Random seed index
  sel = seq(12000, 14000, by = 25),  # MCMC samples for inference
  n = 500,                     # Sample size for analysis
  K = 1000,                    # Number of prediction samples
  iter = 15000,                # Total MCMC iterations
  parallel = TRUE,             # Use parallel processing
  save_exposure_preds = TRUE,
  return_ci = TRUE,
  make_plots = TRUE,
  use_knots = TRUE,            # Use knot-based approximation
  n_knots = 50                 # Number of knots
)

# View results
print(results_basic)
```

**Key Parameters for Basic Usage:**
- `sim_popn`: Wide-format dataset (generated or user-provided)
- `T`: Number of time points
- `sel`: MCMC iterations to use for inference (after burn-in)
- `n`: Sample size for analysis (subsampled from population)
- `K`: Number of prediction samples for g-formula
- `iter`: Total MCMC iterations
- `use_knots`: Whether to use knot-based GP approximation for efficiency
- `n_knots`: Number of knots for approximation

### Advanced Usage (Statistician Mode)

For users who need more control over the modeling process:

```r
# Generate data with different dimensions for advanced example
sim_popn_advanced <- generate_panel_data(
  popN = 1e5,
  T = 4,                      # 4 time points
  Adim = 4,                   # 4 exposures per time point
  Ldim = 2,                   # 2 confounders per time point
  outcome_type = "continuous",
  relationship_type = "quadratic+interaction",
  confounding = "high"
)

# Advanced analysis with custom parameters
results_advanced <- run_gbkmr_panel(
  sim_popn = sim_popn_advanced,
  T = 4,
  currind = 1,
  sel = seq(20000, 25000, by = 30),   # Different sampling scheme
  n = 800,                            # Larger sample size
  K = 2000,                           # More prediction samples
  iter = 30000,                       # More MCMC iterations
  parallel = TRUE,
  save_exposure_preds = TRUE,
  return_ci = TRUE,
  make_plots = TRUE,
  use_knots = TRUE,
  n_knots = 100                       # More knots for complex relationships
)
```

**Advanced Parameters:**
- `T`: Number of time points (must match data generation)
- `sel`: Custom MCMC sampling scheme for inference
- `n`: Analysis sample size (allows subsampling from large populations)
- `K`: Number of prediction samples for g-formula computation
- `iter`: Total MCMC iterations (adjust based on convergence needs)
- `n_knots`: Number of knots (increase for complex exposure-response relationships)

### Using Pre-fitted Models

For maximum flexibility, you can provide pre-fitted BKMR models:

```r
# Fit individual models first (using bkmr package)
library(bkmr)

# Prepare data in wide format for custom fitting
wide_data <- prepare_wide_format(example_data, 
                                time_points = 4, 
                                id = "id", 
                                time = "time")

# Fit custom outcome model
custom_outcome_model <- kmbayes(
  y = wide_data$y_continuous,
  Z = wide_data[, exposure_cols],
  X = wide_data[, covariate_cols],
  iter = 15000,
  varsel = TRUE
)

# Use pre-fitted model in g-BKMR
results_custom <- gbkmr_run(
  data = example_data,
  outcome_model = custom_outcome_model,    # Pre-fitted model
  exposure = "arsenic",
  t_covariates = c("bp", "bmi"),
  base_covariates = c("age", "sex"),
  time_points = 4,
  id = "id",
  time = "time"
)
```

## Results Interpretation

### Main Causal Effect

```r
# Extract main causal effect estimate
causal_effect <- results_basic$diff_gBKMR
cat("Causal Effect (75th vs 25th percentile):", causal_effect, "\n")

# Access counterfactual outcomes
Ya_samples <- results_basic$Ya          # 25th percentile scenario
Yastar_samples <- results_basic$Yastar  # 75th percentile scenario

# Calculate credible intervals
ci_lower <- quantile(Yastar_samples - Ya_samples, 0.025)
ci_upper <- quantile(Yastar_samples - Ya_samples, 0.975)
cat("95% Credible Interval:", ci_lower, "to", ci_upper, "\n")
```

### Visualization

```r
library(ggplot2)

# Plot counterfactual distributions
df_outcomes <- data.frame(
  Scenario = rep(c("25th percentile", "75th percentile"), 
                 c(length(results_basic$Ya), length(results_basic$Yastar))),
  Outcome = c(results_basic$Ya, results_basic$Yastar)
)

ggplot(df_outcomes, aes(x = Outcome, fill = Scenario)) +
  geom_density(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Counterfactual Outcome Distributions",
       x = "Predicted Outcome", y = "Density")

# Plot difference in means
df_plot <- data.frame(
  Scenario = c("25th percentile", "75th percentile"), 
  Mean = c(mean(results_basic$Ya), mean(results_basic$Yastar))
)

ggplot(df_plot, aes(x = Scenario, y = Mean, fill = Scenario)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Counterfactual Means", y = "Mean Outcome")
```

### Accessing Detailed Results

```r
# Access variable importance (beta coefficients)
beta_coefficients <- results_basic$beta_all
cat("Number of coefficients:", length(beta_coefficients), "\n")

# Access mediator predictions
L_values_25th <- results_basic$L_values_a      # Mediator values at 25th percentile
L_values_75th <- results_basic$L_values_astar  # Mediator values at 75th percentile

# Summary of results structure
str(results_basic, max.level = 1)
```

## Model Comparison and Sensitivity Analysis

### Comparing Different Data Generation Scenarios

```r
# Function to compare different relationship types
compare_relationships <- function() {
  relationships <- c("linear", "quadratic", "quadratic+interaction")
  results <- list()
  
  for (rel in relationships) {
    # Generate data
    sim_data <- generate_panel_data(
      popN = 5e4, T = 3, Adim = 3, Ldim = 2,
      outcome_type = "continuous",
      relationship_type = rel,
      confounding = "high"
    )
    
    # Fit model
    fit <- run_gbkmr_panel(
      sim_popn = sim_data, T = 3, currind = 1,
      sel = seq(8000, 10000, by = 25),
      n = 300, iter = 12000, parallel = TRUE,
      make_plots = FALSE
    )
    
    results[[rel]] <- fit$diff_gBKMR
  }
  
  return(results)
}

# Run comparison
relationship_results <- compare_relationships()

# Plot comparison
df_comparison <- data.frame(
  Relationship = names(relationship_results),
  Effect = unlist(relationship_results)
)

ggplot(df_comparison, aes(x = Relationship, y = Effect, fill = Relationship)) +
  geom_col() +
  theme_minimal() +
  labs(title = "g-BKMR Effects Across Different Functional Forms",
       y = "Causal Effect Estimate") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Sensitivity to Sample Size and Dimensions

```r
# Function for comprehensive sensitivity analysis
sensitivity_analysis <- function() {
  # Test different sample sizes
  sample_sizes <- c(200, 500, 1000)
  
  # Test different exposure dimensions
  exposure_dims <- c(2, 3, 5)
  
  # Test different confounder dimensions
  confounder_dims <- c(1, 2, 3)
  
  results <- expand.grid(
    n = sample_sizes,
    Adim = exposure_dims,
    Ldim = confounder_dims
  )
  results$effect <- NA
  
  for (i in 1:nrow(results)) {
    # Generate data
    sim_data <- generate_panel_data(
      popN = max(results$n) * 2,  # Ensure enough population
      T = 3,
      Adim = results$Adim[i],
      Ldim = results$Ldim[i],
      outcome_type = "continuous",
      relationship_type = "quadratic",
      confounding = "high"
    )
    
    # Fit model
    fit <- run_gbkmr_panel(
      sim_popn = sim_data, T = 3, currind = i,
      sel = seq(5000, 7000, by = 25),
      n = results$n[i], iter = 8000, parallel = TRUE,
      make_plots = FALSE
    )
    
    results$effect[i] <- fit$diff_gBKMR
  }
  
  return(results)
}

# Run sensitivity analysis (this may take a while)
# sens_results <- sensitivity_analysis()

# Example of plotting sensitivity results
# ggplot(sens_results, aes(x = n, y = effect, color = factor(Adim))) +
#   geom_line() + geom_point() +
#   facet_wrap(~Ldim, labeller = label_both) +
#   theme_minimal() +
#   labs(title = "Sensitivity Analysis",
#        x = "Sample Size", y = "Causal Effect",
#        color = "Exposure Dimension")
```

## Data Preparation for Real Studies

When working with your own data, you'll need to format it to match the expected structure. The package expects wide-format data with specific naming conventions.

### Data Formatting Guidelines

```r
# Function to help format real data
format_real_data <- function(your_data, T, Adim, Ldim) {
  # Example transformation for longitudinal environmental health data
  
  # Ensure exposure variables are log-transformed and named correctly
  # Expected names: logM1_0, logM2_0, ..., logMAdim_0, logM1_1, ..., logMAdim_T-1
  
  # Ensure confounder variables are named correctly  
  # Expected names: waist1_1, ..., waistLdim_1, ..., waist1_T-1, ..., waistLdim_T-1
  
  # Ensure baseline variables are named correctly
  # Expected names: sex, waist0_1, ..., waist0_Ldim
  
  # Ensure outcome variable is named Y
  
  return(formatted_data)
}

# Validate data structure
validate_gbkmr_data <- function(data, T, Adim, Ldim) {
  required_cols <- c(
    "sex", "Y", "id",
    paste0("waist0_", 1:Ldim),
    unlist(lapply(0:(T-1), function(t) paste0("logM", 1:Adim, "_", t))),
    if (T > 1) unlist(lapply(1:(T-1), function(t) paste0("waist", t, "_", 1:Ldim)))
  )
  
  missing_cols <- setdiff(required_cols, colnames(data))
  if (length(missing_cols) > 0) {
    cat("Missing columns:", paste(missing_cols, collapse = ", "), "\n")
    return(FALSE)
  }
  
  cat("Data validation passed!\n")
  return(TRUE)
}
```

## Performance Optimization

### For Large Datasets

```r
# Optimize for computational efficiency
results_optimized <- gbkmr_run(
  data = large_dataset,
  outcome = "y_binary",
  outcome_type = "binary",
  exposure = "arsenic",
  t_covariates = c("bp", "bmi"),
  base_covariates = c("age", "sex"),
  time_points = 4,
  
  computation = list(
    parallel = TRUE,
    n_cores = 8,              # Use more cores
    use_knots = TRUE,         # Essential for large datasets
    n_knots = 100,            # Increase knots for better approximation
    chunk_size = 1000         # Process data in chunks
  ),
  
  mcmc_control = list(
    iter = 10000,             # Reduce iterations for speed
    burn = 2000,
    thin = 2                  # Use thinning to reduce memory
  )
)
```

### Memory Management

```r
# For memory-constrained environments
results_memory_efficient <- gbkmr_run(
  data = example_data,
  outcome = "y_binary",
  outcome_type = "binary",
  exposure = "arsenic",
  t_covariates = c("bp", "bmi"),
  base_covariates = c("age", "sex"),
  time_points = 4,
  
  memory_control = list(
    save_posterior = FALSE,    # Don't save all posterior samples
    save_models = FALSE,       # Don't save intermediate models
    stream_output = TRUE       # Stream results to disk
  )
)
```

## Real Data Example

Here's an example using a realistic environmental health dataset:

```r
# Load example environmental health data
data("environmental_cohort")  # Example dataset included in package

# Examine data structure
str(environmental_cohort)
head(environmental_cohort)

# Check for missing data
check_missing_data(environmental_cohort)

# Fit g-BKMR model
environmental_results <- gbkmr_run(
  data = environmental_cohort,
  outcome = "cardiovascular_risk",
  outcome_type = "binary",
  exposure = "pm25",                    # Air pollution exposure
  t_covariates = c("blood_pressure", "bmi", "stress_score"),
  base_covariates = c("age", "sex", "education", "smoking_status"),
  time_points = 5,
  id = "participant_id",
  time = "visit_number",
  
  mcmc_control = list(
    iter = 25000,
    burn = 5000
  ),
  
  variable_selection = TRUE,
  
  diagnostics = list(
    trace_plots = TRUE,
    convergence_check = TRUE
  )
)

# Interpret results
summary(environmental_results)
plot(environmental_results, type = "all")

# Export results for publication
export_results(environmental_results, 
               format = "table", 
               file = "gbkmr_results.csv")
```

## Best Practices

### Data Quality

1. **Missing Data**: Handle missing values before analysis
2. **Outliers**: Check for and address extreme values
3. **Temporal Consistency**: Ensure time points are properly ordered
4. **Variable Scaling**: Consider standardizing exposures and covariates

### Model Selection

1. **Start Simple**: Begin with vanilla kernel configuration
2. **Progressive Complexity**: Add variables to kernel as needed
3. **Cross-Validation**: Use cross-validation for model comparison
4. **Domain Knowledge**: Incorporate subject matter expertise

### Computational Considerations

1. **Parallel Processing**: Always enable for datasets with >100 subjects
2. **Knot Selection**: Use more knots for larger datasets
3. **MCMC Tuning**: Monitor convergence and adjust iterations accordingly
4. **Memory Planning**: Consider memory requirements for large datasets

## Troubleshooting

### Common Issues

1. **Convergence Problems**:
   ```r
   # Increase iterations and check trace plots
   results <- gbkmr_run(..., 
                       mcmc_control = list(iter = 50000, burn = 10000),
                       diagnostics = list(trace_plots = TRUE))
   ```

2. **Memory Issues**:
   ```r
   # Use memory-efficient settings
   results <- gbkmr_run(...,
                       computation = list(use_knots = TRUE, n_knots = 50),
                       memory_control = list(save_posterior = FALSE))
   ```

3. **Long Computing Times**:
   ```r
   # Optimize computational settings
   results <- gbkmr_run(...,
                       computation = list(parallel = TRUE, n_cores = 8),
                       mcmc_control = list(iter = 10000))
   ```

## Limitations and Considerations

- **Computational Complexity**: g-BKMR is computationally intensive for large numbers of time points and subjects
- **Model Assumptions**: Relies on standard causal inference assumptions (consistency, exchangeability, positivity)
- **Data Requirements**: Requires complete time series for all subjects
- **Variable Selection**: May be conservative in high-dimensional settings

## Conclusion

The g-BKMR package provides a user-friendly interface for causal inference with time-varying environmental mixtures. The progressive complexity design accommodates both novice users seeking straightforward analysis and expert statisticians requiring detailed model customization. The package's automation of complex longitudinal modeling while maintaining flexibility makes it valuable for environmental health research.
